{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some mnist data\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# -----------------------\n",
    "# 2. Data Loading\n",
    "# -----------------------\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=400, latent_size=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # This is the mu/mean of the latent Gaussian\n",
    "        self.fc_mu = torch.nn.Linear(hidden_size,latent_size)\n",
    "\n",
    "        # This is the logvariance of the latent Gaussian, used instead of standard deviation for numerical stability\n",
    "        self.fc_logvar = torch.nn.Linear(hidden_size,latent_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We apply relu to output so that we do not get negative values, also introduces non-linearity\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        \n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size=20, hidden_size=400, output_size=784):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(latent_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid() # Sigmoid activation function to output values between 0 and 1\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass of decoder: takes latent variable z and returns reconstructed output\n",
    "\n",
    "        Returns:\n",
    "            Image: reconstructed image in [0,1] interval\n",
    "        \"\"\"\n",
    "\n",
    "        hidden = self.relu(self.fc1(z))\n",
    "        output = self.sigmoid(self.fc2(hidden))\n",
    "\n",
    "        return output\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=400, latent_size=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_size, hidden_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, hidden_size, input_size)\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std*eps; eps ~ N(0, I)\n",
    "        \"\"\"\n",
    "        assert mu.size() == logvar.size()\n",
    "        std = torch.exp(0.5 * logvar) # standard deviation\n",
    "        eps = torch.randn_like(std) # random noise mean 0, variance 1 dimension same as std which is latent size\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self,x):\n",
    "        mu,logvar =  self.encoder(x)\n",
    "        z = self.reparameterize(mu,logvar)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "\n",
    "        return x_reconstructed, mu, logvar\n",
    "    \n",
    "    def loss(self, x, x_reconstructed, mu, logvar):\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function\n",
    "        \"\"\"\n",
    "        # Reconstruction loss\n",
    "        BCE = torch.nn.functional.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "\n",
    "        var = logvar.exp()\n",
    "\n",
    "        # KL divergence loss for Gaussian prior and posterior\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - var) # KL divergence between q(z|x) and p(z)\n",
    "\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE().to(device)\n",
    "\n",
    "model.train() # set to training mode\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.view(-1,784).to(device) # gives us a shape of (batch_size, 784)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon, mu, logvar = model(images)\n",
    "        loss = model.loss(images,recon,mu,logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# 6. Testing\n",
    "# -----------------------\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data = data.view(-1, 784).to(device)\n",
    "        recon, mu, logvar = model(data)\n",
    "        loss = model.loss(recon, data, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 7. Visualization\n",
    "# -----------------------\n",
    "# Reconstruct some test images\n",
    "data_iter = iter(test_loader)\n",
    "images, _ = next(data_iter)\n",
    "images = images.to(device).view(-1, 784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon, _, _ = model(images)\n",
    "\n",
    "images = images.view(-1, 28, 28).cpu().numpy()\n",
    "recon = recon.view(-1, 28, 28).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(images[i], cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(recon[i], cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "axes[0, 0].set_title(\"Original\")\n",
    "axes[1, 0].set_title(\"Reconstructed\")\n",
    "plt.show()\n",
    "\n",
    "# Sample from the latent space\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, 20).to(device)\n",
    "    sample = model.decoder(z)\n",
    "    sample = sample.view(-1, 28, 28).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for i in range(16):\n",
    "    r = i // 8\n",
    "    c = i % 8\n",
    "    axes[r, c].imshow(sample[i], cmap='gray')\n",
    "    axes[r, c].axis('off')\n",
    "plt.suptitle(\"Randomly Sampled Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of model\n",
    "torch.save(model.state_dict(), \"vae.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
